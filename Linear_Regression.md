**Regression**-

Regression searches for `relationships` among variables.

For example, you can observe several employees of some company and try to understand how their salaries depend on the features, such as experience, level of education, role, city they work in, and so on.

This is a regression problem where data related to each employee represent one observation. The presumption is that the experience, education, role, and city are the independent features, while the salary depends on them.
***

The *dependent* features are called the `dependent variables`, outputs, or responses.

The *independent* features are called the `independent variables`, inputs, or predictors.

***
Regression problems usually have one continuous and unbounded dependent variable. The inputs, however, can be continuous, discrete, or even categorical data such as gender, nationality, brand, and so on.

It is a common practice to denote the outputs with ğ‘¦ and inputs with ğ‘¥. If there are two or more independent variables, they can be represented as the vector ğ± = (ğ‘¥â‚, â€¦, ğ‘¥áµ£), where ğ‘Ÿ is the number of inputs.
***

*When Do You Need Regression?*-

Typically, you need regression to answer whether and how some phenomenon influences the other or how several variables are related.

For example, you can use it to determine to what extent the experience or gender impact salaries.
***

**Linear Regression** - 

Linear regression is probably one of the most important and widely used regression techniques. Itâ€™s among the simplest regression methods. One of its main advantages is the ease of interpreting results.

*Problem Formulation* - 

When implementing linear regression of some dependent variable ğ‘¦ on the set of independent variables ğ± = (ğ‘¥â‚, â€¦, ğ‘¥áµ£), where ğ‘Ÿ is the number of predictors, you assume a linear relationship between ğ‘¦ and x.

>ğ‘¦ = ğ›½â‚€ + ğ›½â‚ğ‘¥â‚ + â‹¯ + ğ›½áµ£ğ‘¥áµ£ + ğœ€. 

This equation is the `regression equation`. 

where
ğ›½â‚€, ğ›½â‚, â€¦, ğ›½áµ£ are the regression coefficients, and ğœ€ is the random error.

>Linear regression calculates the estimators of the regression coefficients (predicted weights).

Thus,estimated `regression function`,
> ğ‘“(ğ±) = ğ‘â‚€ + ğ‘â‚ğ‘¥â‚ + â‹¯ + ğ‘áµ£ğ‘¥áµ£.

 This function should capture the dependencies between the inputs and output sufficiently well.


let ğ‘¦áµ¢ be the actual response and ğ‘“(ğ±áµ¢) be the predicted(estimated) response.


>The differences ğ‘¦áµ¢ - ğ‘“(ğ±áµ¢) for all observations ğ‘– = 1, â€¦, ğ‘›, are called the `residuals`. 

Regression is about determining the best predicted weights, that is the weights corresponding to the smallest residuals.

*To get the best weights, minimize the `sum of squared residuals (SSR)` for all observations*
 >ğ‘– = 1, â€¦, ğ‘›: SSR = Î£áµ¢(ğ‘¦áµ¢ - ğ‘“(ğ±áµ¢))Â².
 
  This approach is called the method of ordinary least squares.
***


*Regression Performance* -


The coefficient of determination, denoted as ğ‘…Â², tells which amount of variation in ğ‘¦ can be explained by the dependence on ğ± using the particular regression model. 

Larger ğ‘…Â² indicates a better fit and means that the model can better explain the variation of the output with different inputs.

The value ğ‘…Â² = 1 corresponds to SSR = 0, that is to the `perfect fit` since the values of predicted and actual responses fit completely to each other.
***
**Types of Linear Regression** - 

1. `Simple linear Regression` = linear regression with a single independent variable, ğ± = ğ‘¥.

2. `Multiple Linear Regression` = linear regression with two or more independent variables.
3. `Polynomial Regression` =  linear regression in which, *polynomial dependence* between the output and inputs and, consequently, the polynomial estimated regression function.
***

1. **Linear Regression** -

![](https://files.realpython.com/media/fig-lin-reg.a506035b654a.png)

start with a given set of input-output (ğ‘¥-ğ‘¦) pairs (green circles). These pairs are your observations. 

*For example, the leftmost observation (green circle) has the input ğ‘¥ = 5 and the actual output (response) ğ‘¦ = 5. The next one has ğ‘¥ = 15 and ğ‘¦ = 20, and so on.*

*The estimated regression function (black line) has the equation ğ‘“(ğ‘¥) = ğ‘â‚€ + ğ‘â‚ğ‘¥.*

 >goal is to calculate the optimal values of the predicted weights ğ‘â‚€ and ğ‘â‚ that minimize SSR and determine the estimated regression function.
 
  *The value of ğ‘â‚€, also called the intercept, shows the point where the estimated regression line crosses the ğ‘¦ axis. It is the value of the estimated response ğ‘“(ğ‘¥) for ğ‘¥ = 0. The value of ğ‘â‚ determines the slope of the estimated regression line.*

The predicted responses (red squares) are the points on the regression line that correspond to the input values. 

*For example, for the input ğ‘¥ = 5, the predicted response is ğ‘“(5) = 8.33 (represented with the leftmost red square).*

*The residuals (vertical dashed gray lines) can be calculated as*
> ğ‘¦áµ¢ - ğ‘“(ğ±áµ¢) = ğ‘¦áµ¢ - ğ‘â‚€ - ğ‘â‚ğ‘¥áµ¢ for ğ‘– = 1, â€¦, ğ‘›. 

They are the distances between the green circles and red squares. 

`When you implement linear regression, you are actually trying to minimize these distances and make the red squares as close to the predefined green circles as possible`

***
2. **Multiple Linear Regression** - 

If there are just two independent variables, the estimated regression function is 
>ğ‘“(ğ‘¥â‚, ğ‘¥â‚‚) = ğ‘â‚€ + ğ‘â‚ğ‘¥â‚ + ğ‘â‚‚ğ‘¥â‚‚.

 It represents a regression plane in a three-dimensional space. The goal of regression is to determine the values of the weights ğ‘â‚€, ğ‘â‚, and ğ‘â‚‚ such that this plane is as close as possible to the actual responses and yield the minimal SSR.

The case of more than two independent variables is similar, but more general. The estimated regression function is
> ğ‘“(ğ‘¥â‚, â€¦, ğ‘¥áµ£) = ğ‘â‚€ + ğ‘â‚ğ‘¥â‚ + â‹¯ +ğ‘áµ£ğ‘¥áµ£,

 and there are ğ‘Ÿ + 1 weights to be determined when the number of inputs is ğ‘Ÿ.

***
3. **Polynomial Regression** - 

In addition to linear terms like ğ‘â‚ğ‘¥â‚, your regression function ğ‘“ can include non-linear terms such as ğ‘â‚‚ğ‘¥â‚Â², ğ‘â‚ƒğ‘¥â‚Â³, or even ğ‘â‚„ğ‘¥â‚ğ‘¥â‚‚, ğ‘â‚…ğ‘¥â‚Â²ğ‘¥â‚‚, and so on.

The simplest example of polynomial regression has a single independent variable, and the estimated regression function is a polynomial of `degree 2`.
> ğ‘“(ğ‘¥) = ğ‘â‚€ + ğ‘â‚ğ‘¥ + ğ‘â‚‚ğ‘¥Â².

Now,calculate ğ‘â‚€, ğ‘â‚, and ğ‘â‚‚, which minimize SSR. These are unknowns!

Keeping this in mind, compare the previous regression function with the function used for linear regression.
>ğ‘“(ğ‘¥â‚, ğ‘¥â‚‚) = ğ‘â‚€ + ğ‘â‚ğ‘¥â‚ + ğ‘â‚‚ğ‘¥â‚‚
 
 They look very similar and are both linear functions of the unknowns ğ‘â‚€, ğ‘â‚, and ğ‘â‚‚.
This is why we can solve the polynomial regression problem as a linear problem with the term ğ‘¥Â² regarded as an input variable.

*In the case of two variables and the polynomial of degree 2, the regression function has this form.*

 >ğ‘“(ğ‘¥â‚, ğ‘¥â‚‚) = ğ‘â‚€ + ğ‘â‚ğ‘¥â‚ + ğ‘â‚‚ğ‘¥â‚‚ + ğ‘â‚ƒğ‘¥â‚Â² + ğ‘â‚„ğ‘¥â‚ğ‘¥â‚‚ + ğ‘â‚…ğ‘¥â‚‚Â². 
 
 The procedure for solving the problem is identical to the previous case. 
 
 apply linear regression for five inputs: ğ‘¥â‚, ğ‘¥â‚‚, ğ‘¥â‚Â², ğ‘¥â‚ğ‘¥â‚‚, and ğ‘¥â‚‚Â². 
 What we get as the result of regression are the values of six weights which minimize SSR: ğ‘â‚€, ğ‘â‚, ğ‘â‚‚, ğ‘â‚ƒ, ğ‘â‚„, and ğ‘â‚….
***

**Underfitting** - 
underfitting occurs when a model canâ€™t accurately capture the dependencies among data, usually as a consequence of its own simplicity. It often yields a `low ğ‘…Â²` with known data and bad generalization capabilities when applied with new data.

**Overfitting** - 

Overfitting happens when a model learns both dependencies among data and random fluctuations. 

In other words, a model learns the existing data too well. 

Complex models, which have many features or terms, are often prone to overfitting. When applied to known data, such models usually yield `high ğ‘…Â²`. However, they often donâ€™t generalize well and have significantly `lower ğ‘…Â²` when used with new data.

***
